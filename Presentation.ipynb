{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement-based Search Tree Pruning\n",
    "## Abraham Oliver, Brown County High School"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:08:00.625899Z",
     "start_time": "2019-07-21T03:08:00.616496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "import random, sys\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "from copy import copy\n",
    "\n",
    "# Deepnotakto Project\n",
    "from deepnotakto import util, train\n",
    "from deepnotakto.games import notakto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'deepnotakto.games.notakto' from '/home/abe/git/Deep-Notakto/deepnotakto/games/notakto/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(notakto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Game\n",
    "#### 3 x 3\n",
    "**Player 1 Winning Strategy** Play in the center on the first move. Play a knight's move (from chess) from the opponent's move.\n",
    "\n",
    "#### 4 x 4\n",
    "** Player 2 Winning Strategy** Draw an imaginary line either horizontally or vertically, splitting the board in half. Play a knight's move   from the opponent's move on the side of the imaginary line that the opponent's move was played.\n",
    "\n",
    "#### 5 x 5, 6 x 6, and 7x7\n",
    "**Player 1 Winning Strategy** Not yet known\n",
    "\n",
    "#### 8x8 and larger\n",
    "**Winner Not Known**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:19:07.388702Z",
     "start_time": "2019-07-21T03:17:57.717303Z"
    }
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abe/anaconda3/envs/deepnotakto/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3333: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "BOARD_SIZE = 3\n",
    "# Create a human player (that can be used for both players)\n",
    "r = notakto.RandomAgent()\n",
    "h = notakto.Human()\n",
    "# Create a 3x3 game environment\n",
    "e = notakto.Env(BOARD_SIZE)\n",
    "# Play games between the humans on the 3x3\n",
    "gui = notakto.VisualNotaktoGame(e, r, h, -1, show_confidences = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Agents\n",
    "#### Definitions\n",
    "##### Markov Decision Process (MDP)\n",
    "A markov decision process is a decision process that is defined by the tuple $(S, A, R_p(\\cdot), \\gamma)$ where $S$ is a state space (space of possible board positions), $A$ is an action space (space of possible actions for an agent to make), $R_p(s)$ is the immediate reward for some $s \\in S$ and a given player, and $\\gamma$ is the discount factor (the balance between future and immediate rewards). An time step of a deterministic MDP at time $t$ is $(s_t,a_t,r_t)$ where $r_t = R_p(s_t)$. An agent in an MDP is optimized in order to maximize the expected discounted reward from a given time-step $t$ until a terminal state at time-step $T$, $R_T=\\mathbb{E}[\\sum_{n=t}^{T} \\gamma^{n-t} r_t]$.\n",
    "##### Q-Learning\n",
    "In this environment, there exists a function $Q^*: S \\to A$ that produces the action $a$ that maximizes $R_T$ for a given state. Because it is often impossible to find the true $Q^*$, we approximate $Q^*$ with a funtion $Q_\\pi: S \\to A$ that produces an action based on a given policy $\\pi$ (note that $Q=Q^*$ when $\\pi=\\pi^*$, the optimal policy). We define $Q$ by $Q_\\pi(s)=\\mathrm{argmax}_a\\mathbb{E}[R_T|s, a, \\pi]$.\n",
    "##### Q-Agent\n",
    "For a computer agent, Q is defined by a neural network that accepts a given board state and returns a probability distribution over the action space. After a game rollout is completed, we can train the neural network by calculating target Q-values using the Bellman Equation $Q_{\\mathrm{target}}(s_t)=r_t+\\gamma \\mathrm{max}_{a'}Q(s',a')$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:17:00.941221Z",
     "start_time": "2019-07-21T03:15:12.187871Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0727 17:13:15.526562 140633944557376 deprecation.py:323] From /home/abe/git/Deep-Notakto/deepnotakto/QTree.py:184: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "W0727 17:13:15.551548 140633944557376 deprecation_wrapper.py:119] From /home/abe/git/Deep-Notakto/deepnotakto/QTree.py:202: The name tf.verify_tensor_all_finite is deprecated. Please use tf.compat.v1.verify_tensor_all_finite instead.\n",
      "\n",
      "W0727 17:13:15.557926 140633944557376 deprecation_wrapper.py:119] From /home/abe/git/Deep-Notakto/deepnotakto/QTree.py:146: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "W0727 17:13:15.630326 140633944557376 deprecation.py:323] From /home/abe/anaconda3/envs/deepnotakto/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:157: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0727 17:13:15.668594 140633944557376 deprecation_wrapper.py:119] From /home/abe/git/Deep-Notakto/deepnotakto/QTree.py:166: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abe/anaconda3/envs/deepnotakto/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3333: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Create a Q-Agent\n",
    "training_parameters = {\"mode\": \"none\", \"learn_rate\": .01, \"rotate\": True, \"epochs\": 1, \"batch_size\": 2, \"replay_size\": 10}\n",
    "a1 = notakto.QTree(game_size = 3, layers = [9, 9, 9], params = training_parameters)\n",
    "# Create a 3x3 environment\n",
    "e = notakto.Env(3)\n",
    "# Create a random player\n",
    "r = notakto.RandomAgent()\n",
    "# Play the Q-Agent against the random player\n",
    "a1.deterministic = True\n",
    "gui = notakto.VisualNotaktoGame(e, a1, r, -1, show_confidences = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a0bbaa1d70b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load a Q-Agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"agent-saves/notakto/p2_best.npz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotakto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"none\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeterministic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Play the Q-Agent against the random player\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/Deep-Notakto/deepnotakto/util.py\u001b[0m in \u001b[0;36mload_agent\u001b[0;34m(filename, CLASS)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCLASS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mloaded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'layers'"
     ]
    }
   ],
   "source": [
    "# Load a Q-Agent\n",
    "a1 = util.load_agent(\"agent-saves/notakto/p2_best.npz\", notakto.QTree)\n",
    "a1.change_param(\"mode\", \"none\")\n",
    "a1.deterministic = True\n",
    "# Play the Q-Agent against the random player\n",
    "gui = notakto.VisualNotaktoGame(e, a1, h, -1, show_confidences = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:14:56.395621Z",
     "start_time": "2019-07-21T03:14:56.359458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------- QTree(b958b) --------\n",
      "Saved as 'agent-saves/notakto/p2.npz'\n",
      "Started at 5 : 13 PM\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 00 : 06 (at 5 : 13 PM)\n",
      "Iteration             1\n",
      "Q Evaluation          20%\n",
      "BEST MODEL\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 00 : 12 (at 5 : 13 PM)\n",
      "Iteration             2\n",
      "Q Evaluation          3%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 00 : 17 (at 5 : 13 PM)\n",
      "Iteration             3\n",
      "Q Evaluation          0%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 00 : 24 (at 5 : 13 PM)\n",
      "Iteration             4\n",
      "Q Evaluation          3%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 00 : 32 (at 5 : 13 PM)\n",
      "Iteration             5\n",
      "Q Evaluation          0%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 00 : 38 (at 5 : 14 PM)\n",
      "Iteration             6\n",
      "Q Evaluation          3%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 00 : 44 (at 5 : 14 PM)\n",
      "Iteration             7\n",
      "Q Evaluation          6%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 00 : 51 (at 5 : 14 PM)\n",
      "Iteration             8\n",
      "Q Evaluation          23%\n",
      "BEST MODEL\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 00 : 57 (at 5 : 14 PM)\n",
      "Iteration             9\n",
      "Q Evaluation          0%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 01 : 03 (at 5 : 14 PM)\n",
      "Iteration             10\n",
      "Q Evaluation          13%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 01 : 08 (at 5 : 14 PM)\n",
      "Iteration             11\n",
      "Q Evaluation          10%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 01 : 14 (at 5 : 14 PM)\n",
      "Iteration             12\n",
      "Q Evaluation          10%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 01 : 20 (at 5 : 14 PM)\n",
      "Iteration             13\n",
      "Q Evaluation          13%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 01 : 26 (at 5 : 14 PM)\n",
      "Iteration             14\n",
      "Q Evaluation          10%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 01 : 33 (at 5 : 14 PM)\n",
      "Iteration             15\n",
      "Q Evaluation          6%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 01 : 40 (at 5 : 15 PM)\n",
      "Iteration             16\n",
      "Q Evaluation          6%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 01 : 46 (at 5 : 15 PM)\n",
      "Iteration             17\n",
      "Q Evaluation          3%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 01 : 52 (at 5 : 15 PM)\n",
      "Iteration             18\n",
      "Q Evaluation          0%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 01 : 58 (at 5 : 15 PM)\n",
      "Iteration             19\n",
      "Q Evaluation          6%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 02 : 05 (at 5 : 15 PM)\n",
      "Iteration             20\n",
      "Q Evaluation          0%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 02 : 11 (at 5 : 15 PM)\n",
      "Iteration             21\n",
      "Q Evaluation          3%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 02 : 18 (at 5 : 15 PM)\n",
      "Iteration             22\n",
      "Q Evaluation          3%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 02 : 24 (at 5 : 15 PM)\n",
      "Iteration             23\n",
      "Q Evaluation          0%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 02 : 30 (at 5 : 15 PM)\n",
      "Iteration             24\n",
      "Q Evaluation          3%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 02 : 39 (at 5 : 16 PM)\n",
      "Iteration             25\n",
      "Q Evaluation          0%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 02 : 45 (at 5 : 16 PM)\n",
      "Iteration             26\n",
      "Q Evaluation          6%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 02 : 52 (at 5 : 16 PM)\n",
      "Iteration             27\n",
      "Q Evaluation          0%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 03 : 00 (at 5 : 16 PM)\n",
      "Iteration             28\n",
      "Q Evaluation          3%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 03 : 09 (at 5 : 16 PM)\n",
      "Iteration             29\n",
      "Q Evaluation          33%\n",
      "BEST MODEL\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 03 : 16 (at 5 : 16 PM)\n",
      "Iteration             30\n",
      "Q Evaluation          13%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 03 : 23 (at 5 : 16 PM)\n",
      "Iteration             31\n",
      "Q Evaluation          26%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 03 : 30 (at 5 : 16 PM)\n",
      "Iteration             32\n",
      "Q Evaluation          30%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 03 : 37 (at 5 : 17 PM)\n",
      "Iteration             33\n",
      "Q Evaluation          30%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 03 : 44 (at 5 : 17 PM)\n",
      "Iteration             34\n",
      "Q Evaluation          26%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 03 : 54 (at 5 : 17 PM)\n",
      "Iteration             35\n",
      "Q Evaluation          16%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 04 : 01 (at 5 : 17 PM)\n",
      "Iteration             36\n",
      "Q Evaluation          36%\n",
      "BEST MODEL\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 04 : 06 (at 5 : 17 PM)\n",
      "Iteration             37\n",
      "Q Evaluation          6%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 04 : 12 (at 5 : 17 PM)\n",
      "Iteration             38\n",
      "Q Evaluation          13%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 04 : 17 (at 5 : 17 PM)\n",
      "Iteration             39\n",
      "Q Evaluation          6%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 04 : 25 (at 5 : 17 PM)\n",
      "Iteration             40\n",
      "Q Evaluation          16%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 04 : 32 (at 5 : 17 PM)\n",
      "Iteration             41\n",
      "Q Evaluation          0%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 04 : 38 (at 5 : 18 PM)\n",
      "Iteration             42\n",
      "Q Evaluation          0%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 04 : 44 (at 5 : 18 PM)\n",
      "Iteration             43\n",
      "Q Evaluation          13%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 04 : 49 (at 5 : 18 PM)\n",
      "Iteration             44\n",
      "Q Evaluation          10%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 04 : 54 (at 5 : 18 PM)\n",
      "Iteration             45\n",
      "Q Evaluation          10%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 04 : 59 (at 5 : 18 PM)\n",
      "Iteration             46\n",
      "Q Evaluation          20%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 05 : 04 (at 5 : 18 PM)\n",
      "Iteration             47\n",
      "Q Evaluation          10%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 05 : 10 (at 5 : 18 PM)\n",
      "Iteration             48\n",
      "Q Evaluation          0%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 05 : 15 (at 5 : 18 PM)\n",
      "Iteration             49\n",
      "Q Evaluation          6%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 05 : 21 (at 5 : 18 PM)\n",
      "Iteration             50\n",
      "Q Evaluation          0%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 05 : 26 (at 5 : 18 PM)\n",
      "Iteration             51\n",
      "Q Evaluation          0%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 05 : 32 (at 5 : 18 PM)\n",
      "Iteration             52\n",
      "Q Evaluation          0%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 05 : 38 (at 5 : 19 PM)\n",
      "Iteration             53\n",
      "Q Evaluation          0%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 05 : 45 (at 5 : 19 PM)\n",
      "Iteration             54\n",
      "Q Evaluation          3%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 05 : 51 (at 5 : 19 PM)\n",
      "Iteration             55\n",
      "Q Evaluation          13%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 05 : 58 (at 5 : 19 PM)\n",
      "Iteration             56\n",
      "Q Evaluation          10%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 06 : 05 (at 5 : 19 PM)\n",
      "Iteration             57\n",
      "Q Evaluation          33%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 06 : 13 (at 5 : 19 PM)\n",
      "Iteration             58\n",
      "Q Evaluation          16%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 06 : 18 (at 5 : 19 PM)\n",
      "Iteration             59\n",
      "Q Evaluation          3%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 06 : 23 (at 5 : 19 PM)\n",
      "Iteration             60\n",
      "Q Evaluation          13%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 06 : 29 (at 5 : 19 PM)\n",
      "Iteration             61\n",
      "Q Evaluation          6%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 06 : 35 (at 5 : 19 PM)\n",
      "Iteration             62\n",
      "Q Evaluation          13%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 06 : 40 (at 5 : 20 PM)\n",
      "Iteration             63\n",
      "Q Evaluation          16%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 06 : 46 (at 5 : 20 PM)\n",
      "Iteration             64\n",
      "Q Evaluation          16%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 06 : 51 (at 5 : 20 PM)\n",
      "Iteration             65\n",
      "Q Evaluation          10%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 06 : 56 (at 5 : 20 PM)\n",
      "Iteration             66\n",
      "Q Evaluation          6%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 07 : 01 (at 5 : 20 PM)\n",
      "Iteration             67\n",
      "Q Evaluation          3%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 07 : 09 (at 5 : 20 PM)\n",
      "Iteration             68\n",
      "Q Evaluation          6%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 07 : 15 (at 5 : 20 PM)\n",
      "Iteration             69\n",
      "Q Evaluation          10%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 07 : 22 (at 5 : 20 PM)\n",
      "Iteration             70\n",
      "Q Evaluation          13%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 07 : 28 (at 5 : 20 PM)\n",
      "Iteration             71\n",
      "Q Evaluation          16%\n",
      "\n",
      "Self play... Completed\n",
      "Q-based evaluation... Complete\n",
      "Time                  0 : 07 : 34 (at 5 : 20 PM)\n",
      "Iteration             72\n",
      "Q Evaluation          13%\n",
      "\n",
      "Self play... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2ba5a1c55489>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m train.train_model_with_tournament_evaluation(a1, r, e, sims = 70, games = 30, save_every = 10,\n\u001b[1;32m      5\u001b[0m                                             \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"agent-saves/notakto/p2.npz\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                             stats_path = \"agent-saves/notakto/p2.stats\")\n\u001b[0m",
      "\u001b[0;32m~/git/Deep-Notakto/deepnotakto/train.py\u001b[0m in \u001b[0;36mtrain_model_with_tournament_evaluation\u001b[0;34m(agent, opponent, env, statistics, model_path, stats_path, best_model_path, save_every, sims, player, games, break_at_100, console, iter_limit, measure_func)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Self play... \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Generate games\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Train after generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/Deep-Notakto/deepnotakto/QTree.py\u001b[0m in \u001b[0;36mself_play\u001b[0;34m(self, games, simulations, save_every, save_name, train, guided)\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0;31m# Separate node from tree and reset it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseparate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0;31m# Run a guided search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m                 \u001b[0mtree_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mguided\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0;31m# Save the information from this node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/Deep-Notakto/deepnotakto/treesearch.py\u001b[0m in \u001b[0;36mtree_search\u001b[0;34m(root_node, simulations, modified)\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;31m# Backpropagate winner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;31m# If node is forced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m                 \u001b[0;31m# Player that made this position wins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/Deep-Notakto/deepnotakto/games/notakto/notakto.py\u001b[0m in \u001b[0;36maction_space\u001b[0;34m(self, state, remove_losses, remove_isometries, get_probs)\u001b[0m\n\u001b[1;32m     33\u001b[0m                             \u001b[0mremove_isometries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_isometries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                             \u001b[0mget_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_probs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                             probs = probs)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplay_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/Deep-Notakto/deepnotakto/games/notakto/notakto.py\u001b[0m in \u001b[0;36maction_space\u001b[0;34m(state, player, remove_losses, remove_isometries, get_probs, probs)\u001b[0m\n\u001b[1;32m    411\u001b[0m                         \u001b[0;31m# Add all isomorphisms to the remaining arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marray_in_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremain_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m                                 \u001b[0mremain_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marray_in_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremain_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/Deep-Notakto/deepnotakto/util.py\u001b[0m in \u001b[0;36marray_in_list\u001b[0;34m(ar, l)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;34m\"\"\" Is a given numpy array in a list of arrays \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepnotakto/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36marray_equal\u001b[0;34m(a1, a2)\u001b[0m\n\u001b[1;32m   2589\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2590\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepnotakto/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_all\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_count_reduce_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "a1.change_param(\"mode\", \"replay\")\n",
    "a1.deterministic = False\n",
    "train.train_model_with_tournament_evaluation(a1, r, e, sims = 70, games = 30, save_every = 10,\n",
    "                                            model_path = \"agent-saves/notakto/p2.npz\",\n",
    "                                            stats_path = \"agent-saves/notakto/p2.stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Play the newly trained agent\n",
    "a1.change_param(\"mode\", \"none\")\n",
    "a1.deterministic = False\n",
    "gui = notakto.VisualNotaktoGame(e, a1, h, -1, show_confidences = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
