{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy\n",
    "import tensorflow as tf\n",
    "from importlib import reload\n",
    "from random import randint, sample, shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "size = plt.rcParams[\"figure.figsize\"]\n",
    "size[0] = 2\n",
    "size[1] = 2\n",
    "plt.rcParams[\"figure.figsize\"] = size\n",
    "\n",
    "import environment as env\n",
    "import visual\n",
    "import trainer as train\n",
    "import agents.random_plus as rp\n",
    "import agents.Q as Q\n",
    "import agents.human as human\n",
    "import agents.activated as activated\n",
    "import util\n",
    "import agents.perfect as perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = reload(env)\n",
    "_ = reload(Q)\n",
    "_ = reload(visual)\n",
    "_ = reload(activated)\n",
    "_ = reload(util)\n",
    "_ = reload(train)\n",
    "_ = reload(perfect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent\n",
    "agent1 = Q.Q([16, 100, 100, 16], gamma = .6, epsilon = 0.1, beta = 0.0)\n",
    "agent1.load(\"4x4_2.npz\")\n",
    "# agent2 = Q.Q([16, 100, 200, 16], gamma = .6, epsilon = 0.1, beta = 0.0)\n",
    "# agent2.load(\"4x4_server_p2_17.npz\")\n",
    "# Create a random playing agent\n",
    "RandomAgent = rp.RandomAgentPlus()\n",
    "# Create Human agent\n",
    "Human = human.Human()\n",
    "# Create an environment\n",
    "e = env.Env(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gui = visual.GameWithConfidences(e, agent1, Human, -1, piece_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing ********** Done\n"
     ]
    }
   ],
   "source": [
    "e.play(agent1, RandomAgent, 1000, trainer_a1 = train.Trainer(agent1, record = False).get_episode(1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Against Eachother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create both agents\n",
    "a1 = Q.Q([9, 100, 9], gamma = .5, epsilon = 0.1, beta = .01)\n",
    "a2 = Q.Q([9, 100, 9], gamma = .5, epsilon = 0.1, beta = .01)\n",
    "# Create the trainers\n",
    "t1 = train.Trainer(a1)\n",
    "t2 = train.Trainer(a2)\n",
    "# Create the environment\n",
    "train_env = env.Env(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the agents\n",
    "train_env.play(a1, a2, 1000, trainer_a1 = t1.get_episode(rotate = True),\n",
    "               trainer_a2 = t2.get_episode(rotate = True), final_reward = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Watch the agents play\n",
    "gui = visual.GameWithConfidences(train_env, a1, a2, -1, piece_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Watch winning agent play random opponent\n",
    "ra = rp.RandomAgentPlus()\n",
    "gui = visual.GameWithConfidences(train_env, ra, a2, -1, piece_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Play human against winning agent\n",
    "h = human.Human()\n",
    "gui = visual.GameWithConfidences(train_env, a1, h, -1, piece_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Perfect Move Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get move dictionary\n",
    "moves = util.get_move_dict(\"agents/perfect/3.txt\", size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = reload(env)\n",
    "_ = reload(Q)\n",
    "_ = reload(visual)\n",
    "_ = reload(activated)\n",
    "_ = reload(util)\n",
    "_ = reload(train)\n",
    "_ = reload(perfect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = env.Env(5)\n",
    "h = human.Human()\n",
    "r = rp.RandomAgentPlus()\n",
    "a1 = Q.Q([25, 100, 200, 500, 100, 25], gamma = .6, epsilon = 0.1, beta = 0.0)\n",
    "a2 = Q.Q([25, 100, 200, 500, 100, 25], gamma = .6, epsilon = 0.1, beta = 0.0)\n",
    "t1 = train.Trainer(a1, learn_rate = 1e-11, record = False, change_agent_epsilon = True, epsilon_func = lambda x: min(1.0, 10.0 / x))\n",
    "t2 = train.Trainer(a2, learn_rate = 1e-11, record = False, change_agent_epsilon = True, epsilon_func = lambda x: min(1.0, 10.0 / x))\n",
    "p = perfect.Perfect(\"agents/perfect/3.txt\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing ********** Done\n"
     ]
    }
   ],
   "source": [
    "e.play(a1, a2, 100, trainer_a1 = t1.get_episode(), trainer_a2 = t2.get_episode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_perfect_move(perfect_agent, agent_trainer):\n",
    "    state = util.bin_to_array(choice(list(perfect_agent.moves)))\n",
    "    move = perfect_agent.get_move(state)\n",
    "    reward = 10\n",
    "    agent_trainer.offline([state], [move], [reward], silence = True, rotate = True)\n",
    "\n",
    "def train_illegal_move(agent_trainer):\n",
    "    state = np.int32(np.round(np.random.normal(loc = .5, scale = .01, size = [3, 3])))\n",
    "    if np.sum(state) >= 9:\n",
    "        state[0, 0] = 0\n",
    "    # Make illegal move\n",
    "    arg = choice(state.argsort(None)[-int(np.sum(state)):].tolist())\n",
    "    action = np.zeros([state.size], dtype = np.int32)\n",
    "    action[arg] = 1\n",
    "    move = np.reshape(action, state.shape)\n",
    "    reward = -100\n",
    "    t.offline([state], [move], [reward], silence = True, rotate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ITERATIONS = 10000\n",
    "#for i in range(ITERATIONS):\n",
    "#    train_illegal_move(t)\n",
    "for i in range(ITERATIONS):\n",
    "    train_perfect_move(p, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gui = visual.GameWithConfidences(e, a, h, -1, piece_size = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
