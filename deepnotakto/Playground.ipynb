{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy\n",
    "import tensorflow as tf\n",
    "from importlib import reload\n",
    "from random import randint, sample, shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "size = plt.rcParams[\"figure.figsize\"]\n",
    "size[0] = 2\n",
    "size[1] = 2\n",
    "plt.rcParams[\"figure.figsize\"] = size\n",
    "\n",
    "import environment as env\n",
    "import visual\n",
    "import trainer as train\n",
    "import agents.random_plus as rp\n",
    "import agents.Q as Q\n",
    "import agents.human as human\n",
    "import agents.activated as activated\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = reload(env)\n",
    "_ = reload(Q)\n",
    "_ = reload(visual)\n",
    "_ = reload(activated)\n",
    "_ = reload(util)\n",
    "_ = reload(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent\n",
    "agent = Q.Q([16, 100, 100, 16], gamma = .3, epsilon = 0.1, beta = 3.0, name = \"4x4\")\n",
    "trainer = train.Trainer(agent, 1e-8)\n",
    "# Create a random playing agent\n",
    "RandomAgent = rp.RandomAgentPlus()\n",
    "# Create Human agents (separate to keep memories separate)\n",
    "HumanP1 = human.Human()\n",
    "HumanP2 = human.Human()\n",
    "# Create an environment with a 4x4 board\n",
    "e = env.Env(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing ********** Done\n"
     ]
    }
   ],
   "source": [
    "# Train against a random agent in normal mode\n",
    "e.play(RandomAgent, agent, 2000, trainer_a2 = trainer.get_episode(1e-7, rotate = True),\n",
    "       final_reward = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Purpose\n",
    "gui = visual.GameWithConfidences(e, HumanP1, agent, -1, piece_size = 100, episode_train = True, final_reward = True,\n",
    "                                 trainer_a1 = trainer.get_episode(1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2047119140625"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(16*100 + 100*100 + 100*16 + 100 + 100 + 16) / (2 ** 16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
