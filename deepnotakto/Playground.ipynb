{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy\n",
    "import tensorflow as tf\n",
    "from importlib import reload\n",
    "from random import randint, sample\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import environment as env\n",
    "import agent\n",
    "import agents.random_agent as ra\n",
    "import agents.Q as Q\n",
    "import agents.human as human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'agents.human' from 'C:\\\\Users\\\\abeol\\\\Git\\\\Deep-Notakto\\\\deepnotakto\\\\agents\\\\human.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(env)\n",
    "reload(Q)\n",
    "reload(human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ******************** Done\n",
      "Rotating ... Done\n",
      "Training ... Done\n"
     ]
    }
   ],
   "source": [
    "# Create a trainable Q-Learning agent for a 3x3 game\n",
    "QAgent1 = Q.Q([9, 9])\n",
    "# Create a random playing agent\n",
    "QAgent2 = Q.Q([9, 9])\n",
    "# Create a Human Agent\n",
    "Human = human.Human()\n",
    "# Create an environment with a 3x3 board\n",
    "e = env.Env(3, True)\n",
    "# Train the agent over 100 games\n",
    "e.train(QAgent1, QAgent2, 1000, display = False)\n",
    "# Train over same experiences with all rotations and add the rotations to history\n",
    "QAgent1.train_rotate(QAgent1.states, QAgent1.targets, save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-102690\n",
      "-17200\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(QAgent1.rewards))\n",
    "print(np.sum(QAgent2.rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
