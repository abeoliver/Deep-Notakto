{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy\n",
    "import tensorflow as tf\n",
    "from importlib import reload\n",
    "from random import randint, sample, shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "size = plt.rcParams[\"figure.figsize\"]\n",
    "size[0] = 2\n",
    "size[1] = 2\n",
    "plt.rcParams[\"figure.figsize\"] = size\n",
    "\n",
    "import environment as env\n",
    "import visual\n",
    "import trainer\n",
    "import agents.random_agent as rp\n",
    "import agents.Q as Q\n",
    "import agents.human as human\n",
    "import agents.activated as activated\n",
    "import util\n",
    "import agents.agent as agent\n",
    "from train import train_agent\n",
    "# import agents.perfect as perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "_ = reload(env)\n",
    "_ = reload(Q)\n",
    "_ = reload(visual)\n",
    "_ = reload(activated)\n",
    "_ = reload(util)\n",
    "_ = reload(trainer)\n",
    "# _ = reload(perfect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an agent\n",
    "# agent1 = util.load_q_agent(\"agents/params/Q[16, 16]_regular.npz\")\n",
    "agent1 = Q.Q([16, 16])\n",
    "trainer1 = train.Trainer(agent1)\n",
    "# agent2 = Q.Q([16, 100, 200, 16], gamma = .6, epsilon = 0.1, beta = 0.0)\n",
    "# agent2.load(\"4x4_server_p2_17.npz\")\n",
    "# Create a random playing agent\n",
    "RandomAgent = rp.RandomAgentPlus()\n",
    "# Create Human agent\n",
    "Human = human.Human()\n",
    "# Create an environment\n",
    "e = env.Env(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gui = visual.GameWithConfidences(e, agent1, Human, -1, piece_size = 100, trainer_a1 = trainer1.get_online())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e.play(agent1, RandomAgent, 1000, trainer_a1 = train.Trainer(agent1, record = False).get_episode(1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "util.record(\"trials.txt\", agent1, trainer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "util.new_record_file(\"all_trials.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Against Eachother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create both agents\n",
    "a1 = Q.Q([9, 100, 9], gamma = .5, epsilon = 0.1, beta = .01)\n",
    "a2 = Q.Q([9, 100, 9], gamma = .5, epsilon = 0.1, beta = .01)\n",
    "# Create the trainers\n",
    "t1 = train.Trainer(a1)\n",
    "t2 = train.Trainer(a2)\n",
    "# Create the environment\n",
    "train_env = env.Env(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the agents\n",
    "train_env.play(a1, a2, 1000, trainer_a1 = t1.get_episode(rotate = True),\n",
    "               trainer_a2 = t2.get_episode(rotate = True), final_reward = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Watch the agents play\n",
    "gui = visual.GameWithConfidences(train_env, a1, a2, -1, piece_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Watch winning agent play random opponent\n",
    "ra = rp.RandomAgentPlus()\n",
    "gui = visual.GameWithConfidences(train_env, ra, a2, -1, piece_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Play human against winning agent\n",
    "h = human.Human()\n",
    "gui = visual.GameWithConfidences(train_env, a1, h, -1, piece_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Perfect Move Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get move dictionary\n",
    "moves = util.get_move_dict(\"agents/perfect/3.txt\", size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "_ = reload(env)\n",
    "_ = reload(Q)\n",
    "_ = reload(visual)\n",
    "_ = reload(activated)\n",
    "_ = reload(util)\n",
    "_ = reload(train)\n",
    "_ = reload(perfect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = env.Env(5)\n",
    "h = human.Human()\n",
    "r = rp.RandomAgentPlus()\n",
    "a1 = Q.Q([25, 100, 200, 500, 100, 25], gamma = .6, epsilon = 0.1, beta = 0.0)\n",
    "a2 = Q.Q([25, 100, 200, 500, 100, 25], gamma = .6, epsilon = 0.1, beta = 0.0)\n",
    "t1 = train.Trainer(a1, learn_rate = 1e-11, record = False, change_agent_epsilon = True, epsilon_func = lambda x: min(1.0, 10.0 / x))\n",
    "t2 = train.Trainer(a2, learn_rate = 1e-11, record = False, change_agent_epsilon = True, epsilon_func = lambda x: min(1.0, 10.0 / x))\n",
    "p = perfect.Perfect(\"agents/perfect/3.txt\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e.play(a1, a2, 100, trainer_a1 = t1.get_episode(), trainer_a2 = t2.get_episode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_perfect_move(perfect_agent, agent_trainer):\n",
    "    state = util.bin_to_array(choice(list(perfect_agent.moves)))\n",
    "    move = perfect_agent.get_move(state)\n",
    "    reward = 10\n",
    "    agent_trainer.offline([state], [move], [reward], silence = True, rotate = True)\n",
    "\n",
    "def train_illegal_move(agent_trainer):\n",
    "    state = np.int32(np.round(np.random.normal(loc = .5, scale = .01, size = [3, 3])))\n",
    "    if np.sum(state) >= 9:\n",
    "        state[0, 0] = 0\n",
    "    # Make illegal move\n",
    "    arg = choice(state.argsort(None)[-int(np.sum(state)):].tolist())\n",
    "    action = np.zeros([state.size], dtype = np.int32)\n",
    "    action[arg] = 1\n",
    "    move = np.reshape(action, state.shape)\n",
    "    reward = -100\n",
    "    t.offline([state], [move], [reward], silence = True, rotate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = \"THIS IS THE LONGEST NAME\"\n",
    "a = \"[000, 0000, 00000, 00000, 000]\"\n",
    "g = \"00.00\"\n",
    "b = \"00.00\"\n",
    "lr = \"1e-6\"\n",
    "i = \"1000000\"\n",
    "q = \"+1000/1000\"\n",
    "s = \"{name}{nsp}{arch}{asp}{gamma}{gsp}{beta}{bsp}{learn}{lrsp}{iteration}{isp}{wins}\\n\".format(\n",
    "    name = n, arch = a, gamma = g, beta = b, learn = lr, iteration = i, wins = q, nsp = \" \"* (26 - len(n)),\n",
    "    asp = \" \" * (40 - len(a)), gsp = \" \" * (10 - len(g)), bsp = \" \" * (9 - len(b)), lrsp = \" \" * (10 - len(lr)),\n",
    "    isp = \" \" * (13 - len(i))\n",
    "    )\n",
    "print(\"NAME                      ARCHITECTURE                            GAMMA     BETA     L-RATE    ITERS        QUALITY\")\n",
    "print((\"=\" * 120))\n",
    "print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
