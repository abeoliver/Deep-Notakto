{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy\n",
    "import tensorflow as tf\n",
    "from importlib import reload\n",
    "from random import randint, sample, shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "size = plt.rcParams[\"figure.figsize\"]\n",
    "size[0] = 2\n",
    "size[1] = 2\n",
    "plt.rcParams[\"figure.figsize\"] = size\n",
    "\n",
    "import environment as env\n",
    "import visual\n",
    "import trainer as train\n",
    "import agents.random_plus as rp\n",
    "import agents.Q as Q\n",
    "import agents.human as human\n",
    "import agents.activated as activated\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = reload(env)\n",
    "_ = reload(train)\n",
    "_ = reload(Q)\n",
    "_ = reload(visual)\n",
    "_ = reload(activated)\n",
    "_ = reload(util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a trainable Q-Learning agent for a 3x3 game\n",
    "BaseQ = Q.Q([9, 100, 9], gamma = .6, epsilon = .1, beta = 5.0, name = \"PlaygroundTestP1\")\n",
    "tBaseQ = train.Trainer(BaseQ, 1e-4)\n",
    "# Create activated Q-learning agents\n",
    "active = activated.QSigmoidHidden([9, 100, 200, 100, 9], gamma = .6, epsilon = .3, beta = 1.0)\n",
    "tActive = train.Trainer(active, 1e-4)\n",
    "# Create a random playing agent\n",
    "RandomAgent = rp.RandomAgentPlus()\n",
    "# Create Human agents (separate to keep memories separate)\n",
    "HumanP1 = human.Human()\n",
    "HumanP2 = human.Human()\n",
    "# Create an environment with a 3x3 board\n",
    "e = env.Env(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multi-Purpose\n",
    "gui = visual.GameWithConfidences(e, active, HumanP2, -1, piece_size = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collect Human played games\n",
    "gui = visual.GameWithConfidences(e, HumanP1, RandomAgent, max_games = -1, piece_size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, r = [list(HumanP1.states), list(HumanP1.actions), list(HumanP1.rewards)]\n",
    "rotated_s, rotated_a, rotated_r = util.rotate_batch(s, a, r)\n",
    "s.extend(rotated_s)\n",
    "a.extend(rotated_a)\n",
    "r.extend(rotated_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ********** Done\n"
     ]
    }
   ],
   "source": [
    "# Train against previously set human moves\n",
    "tActive.offline(s, a, r, epochs = 10, batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing ********** Done\n"
     ]
    }
   ],
   "source": [
    "# Train against a random agent in normal mode\n",
    "e.play(QAgentP1, RandomAgent, 10000, trainer_a1 = tSigQ.get_online(1e-2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}